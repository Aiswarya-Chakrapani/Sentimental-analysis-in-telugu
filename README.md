# ğŸ“Š Telugu Sentiment Analysis â€“ A Comparative Study of Algorithms

This project performs **sentiment analysis** on Telugu text using both traditional machine learning and deep learning models. The goal is to evaluate and compare the performance of various algorithms in handling sentiment classification in a **low-resource language** like Telugu.

---

## ğŸ§  Abstract

Sentiment analysis in regional languages like Telugu presents challenges due to **limited datasets** and **complex linguistic structures**. This project compares the effectiveness of **seven algorithms**: Logistic Regression, Linear SVM, Decision Tree, Random Forest, LSTM, CNN, and BERT. 

The results highlight that **LSTM** performs the best due to its ability to capture word-order dependencies in Telugu, while **BERT underperformed**, likely due to insufficient pretraining for the Telugu language.

---

## ğŸ“š Dataset Overview

- **Source**: Telugu text from social media and public datasets
- **Train samples**: 23,800  
- **Test samples**: 6,806  
- **Class Distribution**:
  - Positive: 27%
  - Negative: 19%
  - Neutral: 53%

---

## ğŸ§¹ Preprocessing Steps

1. Removed non-Telugu characters
2. Handled missing values
3. Removed duplicates
4. Encoded sentiment labels:
   - Positive â†’ `1`
   - Negative â†’ `-1`
   - Neutral â†’ `0`

---

## ğŸ§ª Feature Engineering

- **TF-IDF Vectorization**
- **Min-Max Scaling**
- **Top Keywords**:
  - "à°¬à°¾à°—à±à°‚à°¦à°¿" (good)
  - "à°šà±†à°¡à±à°¡à°¦à°¿" (bad)
  - "à°¸à°°à°¦à°¾à°—à°¾" (fun)

---

## ğŸ¤– Models Compared

### ğŸ”¸ Machine Learning Models:
- **Logistic Regression** â€“ Baseline model, 58.99% accuracy
- **Linear SVM (SGD)** â€“ Similar performance, 58.58% accuracy
- **Decision Tree** â€“ 51% accuracy; struggles with agglutinative nature of Telugu
- **Random Forest** â€“ 54.5% accuracy

### ğŸ”¸ Deep Learning Models:
- **LSTM** â€“ Best performing model, **61.43% accuracy**
- **CNN** â€“ Underperformed, 52% accuracy

### ğŸ”¸ Transformer Model:
- **BERT** â€“ 54.31% accuracy; shows limitations of transfer learning in low-resource languages

---

## ğŸ“Š Results Summary

| Algorithm       | Accuracy |
|----------------|----------|
| Logistic Regression | 58.99% |
| Linear SVM          | 58.58% |
| Decision Tree       | 51.00% |
| Random Forest       | 54.50% |
| CNN                 | 52.00% |
| **LSTM**            | **61.43%** |
| BERT                | 54.31% |

LSTM clearly outperforms due to its ability to model sequence and context effectively in Telugu.

---

## ğŸ“ˆ Visuals

The project includes:
- Sentence Length Histograms
- Sentiment Label Distributions
- Top 20 Frequent Words
- Accuracy/Recall Charts

---


## ğŸ”š Conclusion

This study provides critical insight into the **efficacy of various algorithms** for sentiment analysis in Telugu. LSTM is most effective, while models like BERT show potential with better language-specific pretraining.

---

